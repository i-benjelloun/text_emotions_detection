{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_Baseline_Modeling.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPb5aj+w0o2m+ZYyWpTkR7n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KaNlFWOY2W3M"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"B2CWRIBh8ekZ"},"source":["\n","\n","*   In the previous step, we have cleanded our data\n","*   In this notebook, we will try to build a baseline model that detects one or multiple emotions in a text based on the GoEmotions data (multi-label text classification)\n","*   The score of pur baseline model will be used as a reference when building more complex models\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bmJO_oLA2ejV"},"source":["# 1 - Importing libraries and loading data"]},{"cell_type":"markdown","metadata":{"id":"Bn46rnObNtV_"},"source":["First, let's install and import some libraries for data exploration and  processing."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OGeR81nwhnZs","executionInfo":{"status":"ok","timestamp":1617217205862,"user_tz":-120,"elapsed":7628,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"7b18e067-2736-4488-c421-5ba165e7a652"},"source":["# Installing additional libraries for text preprocessing\n","!pip install emoji\n","!pip install contractions"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n","Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.48)\n","Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n","Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.1.7)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jjd5oso_tAwP"},"source":["# Data manipulation libraries\n","import pandas as pd\n","import numpy as np\n","import json\n","from pprint import pprint\n","\n","# Text processing libraries\n","import emoji\n","import re\n","import contractions\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","# Scikit-Learn packages\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6c5SahGbsAA","executionInfo":{"status":"ok","timestamp":1617217206919,"user_tz":-120,"elapsed":8662,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"6d5e0d13-d2f5-446c-df13-e03fb55418ac"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pFJTxr7i-jgO"},"source":["Now, let's import our data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h2mHjyEm-fyv","executionInfo":{"status":"ok","timestamp":1617217207222,"user_tz":-120,"elapsed":8955,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"79dec385-d859-4645-d8f1-cdff90decbce"},"source":["# Importing train, validation and test datasets with preprocessed texts and labels\n","train_GE = pd.read_csv(\"/content/drive/MyDrive/GoEmotions_Git/data/train_clean.csv\")\n","val_GE = pd.read_csv(\"/content/drive/MyDrive/GoEmotions_Git/data/val_clean.csv\")\n","test_GE = pd.read_csv(\"/content/drive/MyDrive/GoEmotions_Git/data/test_clean.csv\")\n","\n","# Shape validation\n","print(train_GE.shape)\n","print(val_GE.shape)\n","print(test_GE.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(43410, 29)\n","(5426, 29)\n","(5427, 29)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t5L92Ke6_To-","executionInfo":{"status":"ok","timestamp":1617217207223,"user_tz":-120,"elapsed":8946,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"dab891ef-77b7-4f81-dfe3-b7e261e94040"},"source":["# Loading emotion labels for GoEmotions taxonomy\n","with open(\"/content/drive/MyDrive/GoEmotions_Git/data/emotions.txt\", \"r\") as file:\n","    GE_taxonomy = file.read().split(\"\\n\")\n","\n","for emo in GE_taxonomy:\n","  print(emo)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["admiration\n","amusement\n","anger\n","annoyance\n","approval\n","caring\n","confusion\n","curiosity\n","desire\n","disappointment\n","disapproval\n","disgust\n","embarrassment\n","excitement\n","fear\n","gratitude\n","grief\n","joy\n","love\n","nervousness\n","optimism\n","pride\n","realization\n","relief\n","remorse\n","sadness\n","surprise\n","neutral\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l2KyRDetNXm5"},"source":["# 2 - Preprocessings and transformations"]},{"cell_type":"markdown","metadata":{"id":"Yde1mZgyN1wj"},"source":["Before defining and constructing a baseline model, we need to perform some additional processings such as tokenizing and lemmatizing our samples."]},{"cell_type":"markdown","metadata":{"id":"F4YpoidgOZAH"},"source":["## 2.1 - Additional preprocessings for basic Machine Learning tasks"]},{"cell_type":"markdown","metadata":{"id":"lQaFs5AQOj_e"},"source":["First, let's remove all punctuations."]},{"cell_type":"code","metadata":{"id":"eVUDXY5tO9ZJ"},"source":["# Additional text preprocessing\n","train_GE['Clean_text'] = train_GE['Clean_text'].apply(lambda x: re.sub(r\"[^A-Za-z_]+\",\" \", x))\n","test_GE['Clean_text'] = test_GE['Clean_text'].apply(lambda x: re.sub(r\"[^A-Za-z_]+\",\" \", x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWCcHThxPdUA"},"source":["New we can tokenize our samples using spacy and more specifically the english model. After creating these tokens, we will be able to lemmatize them and remove english stop words that may not help us in the classification task."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vcznKVRpOu2v","executionInfo":{"status":"ok","timestamp":1617217212303,"user_tz":-120,"elapsed":14010,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"cf78ba50-63d4-49db-b927-c2d65f5cd5ab"},"source":["# Download model \n","!python -m spacy download en_core_web_sm -q"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bc_hi-ipP_dE"},"source":["# Import English using en_core_web_sm.load()\n","import en_core_web_sm\n","nlp = en_core_web_sm.load()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wiAEGmEqQD98"},"source":["# Creating tokenized documents\n","tokenized_train_GE = train_GE[\"Clean_text\"].apply(lambda desc: nlp(desc))\n","tokenized_test_GE = test_GE[\"Clean_text\"].apply(lambda desc: nlp(desc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXy9aEC3QVmd"},"source":["# Lemmatize each token and removing english stopwords\n","tokenized_train_GE = tokenized_train_GE.apply(lambda x: [token.lemma_ for token in x if token.lemma_ not in STOP_WORDS])\n","tokenized_test_GE = tokenized_test_GE.apply(lambda x: [token.lemma_ for token in x if token.lemma_ not in STOP_WORDS])\n","\n","# Creating clean data in our dataframes\n","train_GE[\"Clean_token\"] = [\" \".join(x) for x in tokenized_train_GE]\n","test_GE[\"Clean_token\"] = [\" \".join(x) for x in tokenized_test_GE]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"reuWDr-ISwcd"},"source":["## 2.2 - Create TF-IDF matrix"]},{"cell_type":"markdown","metadata":{"id":"COHnSzRvS83t"},"source":["Finally, we can create a TF-IDF matrix that will help us represent each sample of our corpus using the importance and frequency of each word in the sample, but also in the whole corpus."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITZzFL0KUqJa","executionInfo":{"status":"ok","timestamp":1617217770193,"user_tz":-120,"elapsed":571867,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"dd9d9c67-1a4f-4ff1-acb1-03ee68111ea1"},"source":["# TF-IDF vector with 1000 words vocabulary \n","vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000)\n","\n","# Fitting the vectorizer and transforming train and test data\n","tfidf_train_GE = vectorizer.fit_transform(train_GE['Clean_token'])\n","tfidf_test_GE = vectorizer.transform(test_GE['Clean_token'])\n","\n","# Transforming from generators to arrays\n","tfidf_train_GE = tfidf_train_GE.toarray()\n","tfidf_test_GE = tfidf_test_GE.toarray()\n","\n","# Validating the shape of train and test data\n","print(tfidf_train_GE.shape)\n","print(tfidf_test_GE.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(43410, 1000)\n","(5427, 1000)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SfwfHUghUqI_"},"source":["The `max_features` argument in the `TfidfVectorizer` allows the maximum number of words to be considered in the vocabulary. Therefore, each sample in the train and test datasets will be represented using a vector of dimension `(1,1000)`.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dAr15JAoVY0A"},"source":["## 2.3 - Train and test variables"]},{"cell_type":"markdown","metadata":{"id":"eWLKT7opVgmY"},"source":["Let's define some explicit variables that will be used in constructing a machine learning model."]},{"cell_type":"code","metadata":{"id":"bIINuxk7KZxJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617217770194,"user_tz":-120,"elapsed":571860,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"bb052e3c-499d-48f2-a4c5-aca60860d8b5"},"source":["# Defining train and test variables\n","X_train =  tfidf_train_GE\n","y_train = train_GE.loc[:,GE_taxonomy].values\n","\n","X_test =  tfidf_test_GE\n","y_test = test_GE.loc[:,GE_taxonomy].values\n","\n","# Shape validation\n","print(\"The shape of X_train is : \", X_train.shape)\n","print(\"The shape of y_train is : \", y_train.shape)\n","print()\n","print(\"The shape of X_test is : \", X_test.shape)\n","print(\"The shape of y_test is : \", y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The shape of X_train is :  (43410, 1000)\n","The shape of y_train is :  (43410, 28)\n","\n","The shape of X_test is :  (5427, 1000)\n","The shape of y_test is :  (5427, 28)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6kYzU4f8V9Za"},"source":["# 3 - Dummy model"]},{"cell_type":"markdown","metadata":{"id":"HTC3soTRYkeT"},"source":["## 3.1 - Simulating dummy predictions"]},{"cell_type":"markdown","metadata":{"id":"AG6-PL7UWCi2"},"source":["Before creating a baseline model, we can try and simulate a **\"dummy model\"** that will **always detect the same emotions**, regardless of the sample. In our case, the dummy model could always predict the **'Neutral'** emotion as it is the most represented class in our train dataset."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":315},"id":"zlY_dBVjXZdk","executionInfo":{"status":"ok","timestamp":1617217770195,"user_tz":-120,"elapsed":571851,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"c64e6755-73e8-453a-80b5-e2807b47bf7b"},"source":["# Preview of data\n","display(train_GE.head(3))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Clean_text</th>\n","      <th>admiration</th>\n","      <th>amusement</th>\n","      <th>anger</th>\n","      <th>annoyance</th>\n","      <th>approval</th>\n","      <th>caring</th>\n","      <th>confusion</th>\n","      <th>curiosity</th>\n","      <th>desire</th>\n","      <th>disappointment</th>\n","      <th>disapproval</th>\n","      <th>disgust</th>\n","      <th>embarrassment</th>\n","      <th>excitement</th>\n","      <th>fear</th>\n","      <th>gratitude</th>\n","      <th>grief</th>\n","      <th>joy</th>\n","      <th>love</th>\n","      <th>nervousness</th>\n","      <th>optimism</th>\n","      <th>pride</th>\n","      <th>realization</th>\n","      <th>relief</th>\n","      <th>remorse</th>\n","      <th>sadness</th>\n","      <th>surprise</th>\n","      <th>neutral</th>\n","      <th>Clean_token</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>my favourite food is anything i did not have t...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>-PRON- favourite food cook -PRON-</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>now if he does off himself everyone will think...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>-PRON- -PRON- think -PRON- s laugh screw peopl...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>why the fuck is bayless isoing</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>fuck bayles isoe</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          Clean_text  ...                                        Clean_token\n","0  my favourite food is anything i did not have t...  ...                  -PRON- favourite food cook -PRON-\n","1  now if he does off himself everyone will think...  ...  -PRON- -PRON- think -PRON- s laugh screw peopl...\n","2                     why the fuck is bayless isoing  ...                                   fuck bayles isoe\n","\n","[3 rows x 30 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"GClM7Si8W2tr"},"source":["Without training an actual model, we can directly generate a predicitions matrice that mimics such a behaviour. The 'Neutral' emotion is the last emotion in our `GE_Taxonomy`list, therefore, it is also the last column in `y_train` and `y_test`."]},{"cell_type":"code","metadata":{"id":"Ou3jw2pwI7XA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617217770195,"user_tz":-120,"elapsed":571842,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"e4e48b81-910e-4643-ebc3-278c1632c949"},"source":["# Always predicting neutral emotion \n","dummy_preds = np.zeros_like(y_test)\n","dummy_preds[:,-1] = 1\n","dummy_preds"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 0, ..., 0, 0, 1],\n","       [0, 0, 0, ..., 0, 0, 1],\n","       [0, 0, 0, ..., 0, 0, 1],\n","       ...,\n","       [0, 0, 0, ..., 0, 0, 1],\n","       [0, 0, 0, ..., 0, 0, 1],\n","       [0, 0, 0, ..., 0, 0, 1]])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"fPDoeXNiYqTO"},"source":["## 3.2 - Evaluation on GoEmotions taxonomy"]},{"cell_type":"markdown","metadata":{"id":"3lC5GGd1ZTWz"},"source":["In order to evaluate the model, we will be using the f1-score. The f1-score allows to balance between recall and precision, which is very useful when it comes to unbalanced data."]},{"cell_type":"markdown","metadata":{"id":"LLFvYSmhZzZM"},"source":["We define a custom function that will compute the f1-score, precision and recall for each emotion, and also compute the macro-average of these metrics as a global metric."]},{"cell_type":"code","metadata":{"id":"3CJMgmzj2XoN"},"source":["# Model evaluation function \n","def model_eval(y_true, y_pred_labels, emotions):\n","    \n","    # Defining variables\n","    precision = []\n","    recall = []\n","    f1 = []\n","    \n","    # Per emotion evaluation      \n","    idx2emotion = {i: e for i, e in enumerate(emotions)}\n","    \n","    for i in range(len(emotions)):\n","   \n","        # Computing precision, recall and f1-score\n","        p, r, f1_score, _ = precision_recall_fscore_support(y_true[:, i], y_pred_labels[:, i], average=\"binary\")\n","        \n","        # Append results in lists\n","        precision.append(round(p, 2))\n","        recall.append(round(r, 2))\n","        f1.append(round(f1_score, 2))\n","    \n","    # Macro evaluation\n","    macro_p, macro_r, macro_f1_score, _ = precision_recall_fscore_support(y_true, y_pred_labels, average=\"macro\")\n","    \n","    # Append results in lists\n","    precision.append(round(macro_p, 2))\n","    recall.append(round(macro_r, 2))\n","    f1.append(round(macro_f1_score, 2))\n","    \n","    # Converting results to a dataframe\n","    df_results = pd.DataFrame({\"Precision\":precision, \"Recall\":recall, 'F1':f1})\n","    df_results.index = emotions+['MACRO-AVERAGE']\n","    \n","    return df_results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"rW_rNrE_cN56","executionInfo":{"status":"ok","timestamp":1617217770583,"user_tz":-120,"elapsed":572214,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"b6cbe288-d159-4681-9ca2-9e2be87fce39"},"source":["# Model evaluation\n","model_eval(y_test, dummy_preds, GE_taxonomy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>admiration</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>amusement</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>anger</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>annoyance</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>approval</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>caring</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>confusion</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>curiosity</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>desire</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>disappointment</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>disapproval</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>disgust</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>embarrassment</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>excitement</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>fear</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>gratitude</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>grief</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>joy</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>love</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>nervousness</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>optimism</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>pride</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>realization</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>relief</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>remorse</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>sadness</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>surprise</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>neutral</th>\n","      <td>0.33</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>MACRO-AVERAGE</th>\n","      <td>0.01</td>\n","      <td>0.04</td>\n","      <td>0.02</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                Precision  Recall    F1\n","admiration           0.00    0.00  0.00\n","amusement            0.00    0.00  0.00\n","anger                0.00    0.00  0.00\n","annoyance            0.00    0.00  0.00\n","approval             0.00    0.00  0.00\n","caring               0.00    0.00  0.00\n","confusion            0.00    0.00  0.00\n","curiosity            0.00    0.00  0.00\n","desire               0.00    0.00  0.00\n","disappointment       0.00    0.00  0.00\n","disapproval          0.00    0.00  0.00\n","disgust              0.00    0.00  0.00\n","embarrassment        0.00    0.00  0.00\n","excitement           0.00    0.00  0.00\n","fear                 0.00    0.00  0.00\n","gratitude            0.00    0.00  0.00\n","grief                0.00    0.00  0.00\n","joy                  0.00    0.00  0.00\n","love                 0.00    0.00  0.00\n","nervousness          0.00    0.00  0.00\n","optimism             0.00    0.00  0.00\n","pride                0.00    0.00  0.00\n","realization          0.00    0.00  0.00\n","relief               0.00    0.00  0.00\n","remorse              0.00    0.00  0.00\n","sadness              0.00    0.00  0.00\n","surprise             0.00    0.00  0.00\n","neutral              0.33    1.00  0.50\n","MACRO-AVERAGE        0.01    0.04  0.02"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"MygPwjp6dKxC"},"source":["As expected, **the model performs very poorly**. However, we can try to improve this score by implementing a baseline model using a simple machine learning classification model."]},{"cell_type":"markdown","metadata":{"id":"ehWZp50ReWv1"},"source":["# 4 - Baseline model: Ridge Classifier"]},{"cell_type":"markdown","metadata":{"id":"4YyEdLSde6-Y"},"source":["In this section, we will train a simple classification algorithm, the ridge classifier. However, this algorithm does not support multi-label classification. A simple strategy to do that consists of fitting one model per target using the `MultiOutputClassifier`."]},{"cell_type":"markdown","metadata":{"id":"ymA6YBshf49b"},"source":["## 4.1 - Training the model and evaluation on GoEmotions taxonomy"]},{"cell_type":"markdown","metadata":{"id":"1hfQ1tXjgJwt"},"source":["Let's create our model and fit it to our data. This a pretty simple model as it converts target variables to {-1,1} and trats the problem as a regular regression task."]},{"cell_type":"code","metadata":{"id":"uZRj1Pt5VI4C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617217842890,"user_tz":-120,"elapsed":644509,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"0c7ae13c-abec-4167-e34c-c16bd7452edc"},"source":["# Multi-label classification \n","rc = RidgeClassifier(class_weight='balanced')\n","classifier = MultiOutputClassifier(rc, n_jobs=-1)\n","classifier.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  \"timeout or by a memory leak.\", UserWarning\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["MultiOutputClassifier(estimator=RidgeClassifier(alpha=1.0,\n","                                                class_weight='balanced',\n","                                                copy_X=True, fit_intercept=True,\n","                                                max_iter=None, normalize=False,\n","                                                random_state=None,\n","                                                solver='auto', tol=0.001),\n","                      n_jobs=-1)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5KFEM7vJTTNn","executionInfo":{"status":"ok","timestamp":1617217844617,"user_tz":-120,"elapsed":646226,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"f390aae1-0439-4d9d-fa64-8bdd9099691d"},"source":["# Making predictions on GoEmotions taxonomy \n","classifier_preds = classifier.predict(X_test)\n","classifier_preds"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 0, ..., 1, 0, 0],\n","       [1, 0, 0, ..., 0, 0, 0],\n","       [1, 0, 0, ..., 0, 0, 0],\n","       ...,\n","       [0, 0, 0, ..., 0, 0, 1],\n","       [1, 0, 0, ..., 0, 0, 0],\n","       [0, 1, 0, ..., 0, 0, 1]])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":948},"id":"jg-VzcqFsic-","executionInfo":{"status":"ok","timestamp":1617217890782,"user_tz":-120,"elapsed":485,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"ed99cd03-c8af-4dc4-8ead-abb445fe0133"},"source":["# Model evaluation\n","model_eval(y_test, classifier_preds, GE_taxonomy)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>admiration</th>\n","      <td>0.38</td>\n","      <td>0.71</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>amusement</th>\n","      <td>0.58</td>\n","      <td>0.91</td>\n","      <td>0.71</td>\n","    </tr>\n","    <tr>\n","      <th>anger</th>\n","      <td>0.15</td>\n","      <td>0.65</td>\n","      <td>0.24</td>\n","    </tr>\n","    <tr>\n","      <th>annoyance</th>\n","      <td>0.14</td>\n","      <td>0.61</td>\n","      <td>0.23</td>\n","    </tr>\n","    <tr>\n","      <th>approval</th>\n","      <td>0.15</td>\n","      <td>0.67</td>\n","      <td>0.24</td>\n","    </tr>\n","    <tr>\n","      <th>caring</th>\n","      <td>0.07</td>\n","      <td>0.59</td>\n","      <td>0.13</td>\n","    </tr>\n","    <tr>\n","      <th>confusion</th>\n","      <td>0.07</td>\n","      <td>0.61</td>\n","      <td>0.13</td>\n","    </tr>\n","    <tr>\n","      <th>curiosity</th>\n","      <td>0.08</td>\n","      <td>0.53</td>\n","      <td>0.15</td>\n","    </tr>\n","    <tr>\n","      <th>desire</th>\n","      <td>0.11</td>\n","      <td>0.69</td>\n","      <td>0.18</td>\n","    </tr>\n","    <tr>\n","      <th>disappointment</th>\n","      <td>0.06</td>\n","      <td>0.51</td>\n","      <td>0.10</td>\n","    </tr>\n","    <tr>\n","      <th>disapproval</th>\n","      <td>0.09</td>\n","      <td>0.58</td>\n","      <td>0.15</td>\n","    </tr>\n","    <tr>\n","      <th>disgust</th>\n","      <td>0.10</td>\n","      <td>0.61</td>\n","      <td>0.18</td>\n","    </tr>\n","    <tr>\n","      <th>embarrassment</th>\n","      <td>0.03</td>\n","      <td>0.51</td>\n","      <td>0.05</td>\n","    </tr>\n","    <tr>\n","      <th>excitement</th>\n","      <td>0.07</td>\n","      <td>0.61</td>\n","      <td>0.13</td>\n","    </tr>\n","    <tr>\n","      <th>fear</th>\n","      <td>0.11</td>\n","      <td>0.71</td>\n","      <td>0.20</td>\n","    </tr>\n","    <tr>\n","      <th>gratitude</th>\n","      <td>0.69</td>\n","      <td>0.91</td>\n","      <td>0.78</td>\n","    </tr>\n","    <tr>\n","      <th>grief</th>\n","      <td>0.02</td>\n","      <td>0.83</td>\n","      <td>0.05</td>\n","    </tr>\n","    <tr>\n","      <th>joy</th>\n","      <td>0.18</td>\n","      <td>0.77</td>\n","      <td>0.29</td>\n","    </tr>\n","    <tr>\n","      <th>love</th>\n","      <td>0.40</td>\n","      <td>0.83</td>\n","      <td>0.54</td>\n","    </tr>\n","    <tr>\n","      <th>nervousness</th>\n","      <td>0.02</td>\n","      <td>0.39</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>optimism</th>\n","      <td>0.16</td>\n","      <td>0.70</td>\n","      <td>0.26</td>\n","    </tr>\n","    <tr>\n","      <th>pride</th>\n","      <td>0.02</td>\n","      <td>0.50</td>\n","      <td>0.04</td>\n","    </tr>\n","    <tr>\n","      <th>realization</th>\n","      <td>0.06</td>\n","      <td>0.55</td>\n","      <td>0.11</td>\n","    </tr>\n","    <tr>\n","      <th>relief</th>\n","      <td>0.01</td>\n","      <td>0.27</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>remorse</th>\n","      <td>0.18</td>\n","      <td>0.80</td>\n","      <td>0.30</td>\n","    </tr>\n","    <tr>\n","      <th>sadness</th>\n","      <td>0.14</td>\n","      <td>0.70</td>\n","      <td>0.24</td>\n","    </tr>\n","    <tr>\n","      <th>surprise</th>\n","      <td>0.16</td>\n","      <td>0.69</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>neutral</th>\n","      <td>0.53</td>\n","      <td>0.81</td>\n","      <td>0.64</td>\n","    </tr>\n","    <tr>\n","      <th>MACRO-AVERAGE</th>\n","      <td>0.17</td>\n","      <td>0.65</td>\n","      <td>0.24</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                Precision  Recall    F1\n","admiration           0.38    0.71  0.50\n","amusement            0.58    0.91  0.71\n","anger                0.15    0.65  0.24\n","annoyance            0.14    0.61  0.23\n","approval             0.15    0.67  0.24\n","caring               0.07    0.59  0.13\n","confusion            0.07    0.61  0.13\n","curiosity            0.08    0.53  0.15\n","desire               0.11    0.69  0.18\n","disappointment       0.06    0.51  0.10\n","disapproval          0.09    0.58  0.15\n","disgust              0.10    0.61  0.18\n","embarrassment        0.03    0.51  0.05\n","excitement           0.07    0.61  0.13\n","fear                 0.11    0.71  0.20\n","gratitude            0.69    0.91  0.78\n","grief                0.02    0.83  0.05\n","joy                  0.18    0.77  0.29\n","love                 0.40    0.83  0.54\n","nervousness          0.02    0.39  0.03\n","optimism             0.16    0.70  0.26\n","pride                0.02    0.50  0.04\n","realization          0.06    0.55  0.11\n","relief               0.01    0.27  0.01\n","remorse              0.18    0.80  0.30\n","sadness              0.14    0.70  0.24\n","surprise             0.16    0.69  0.25\n","neutral              0.53    0.81  0.64\n","MACRO-AVERAGE        0.17    0.65  0.24"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"qeCbA0_Lsu_K"},"source":["As we can see, our baseline model performs better than the dummy model. However, the score is still relatively low and can be improved using a more advanced model. This score will be used as a reference in the next steps."]},{"cell_type":"markdown","metadata":{"id":"zjMHKBd6pEl6"},"source":["## 4.2 - Make predictions"]},{"cell_type":"markdown","metadata":{"id":"sJABBqRVtlbk"},"source":["To make predictions on a new sample, it needs to be processed using all the different precessing steps we used."]},{"cell_type":"code","metadata":{"id":"tdTRJR_ZhVKI"},"source":["# Retrieving initial text preprocessings\n","def preprocess_corpus(x):\n","    \n","    # Adding a space between words and punctation\n","    x = re.sub( r'([a-zA-Z\\[\\]])([,;.!?])', r'\\1 \\2', x)\n","    x = re.sub( r'([,;.!?])([a-zA-Z\\[\\]])', r'\\1 \\2', x)\n","\n","    # Demojize\n","    x = emoji.demojize(x)\n","\n","    # Expand contraction\n","    x = contractions.fix(x)\n","\n","    # Lower\n","    x = x.lower()\n","\n","    #correct some acronyms/typos/abbreviations  \n","    x = re.sub(r\"lmao\", \"laughing my ass off\", x)  \n","    x = re.sub(r\"amirite\", \"am i right\", x)\n","    x = re.sub(r\"\\b(tho)\\b\", \"though\", x)\n","    x = re.sub(r\"\\b(ikr)\\b\", \"i know right\", x)\n","    x = re.sub(r\"\\b(ya|u)\\b\", \"you\", x)\n","    x = re.sub(r\"\\b(eu)\\b\", \"europe\", x)\n","    x = re.sub(r\"\\b(da)\\b\", \"the\", x)\n","    x = re.sub(r\"\\b(dat)\\b\", \"that\", x)\n","    x = re.sub(r\"\\b(dats)\\b\", \"that is\", x)\n","    x = re.sub(r\"\\b(cuz)\\b\", \"because\", x)\n","    x = re.sub(r\"\\b(fkn)\\b\", \"fucking\", x)\n","    x = re.sub(r\"\\b(tbh)\\b\", \"to be honest\", x)\n","    x = re.sub(r\"\\b(tbf)\\b\", \"to be fair\", x)\n","    x = re.sub(r\"faux pas\", \"mistake\", x)\n","    x = re.sub(r\"\\b(btw)\\b\", \"by the way\", x)\n","    x = re.sub(r\"\\b(bs)\\b\", \"bullshit\", x)\n","    x = re.sub(r\"\\b(kinda)\\b\", \"kind of\", x)\n","    x = re.sub(r\"\\b(bruh)\\b\", \"bro\", x)\n","    x = re.sub(r\"\\b(w/e)\\b\", \"whatever\", x)\n","    x = re.sub(r\"\\b(w/)\\b\", \"with\", x)\n","    x = re.sub(r\"\\b(w/o)\\b\", \"without\", x)\n","    x = re.sub(r\"\\b(doj)\\b\", \"department of justice\", x)\n","\n","    # replace some words with multiple occurences of a letter, example \"coooool\" turns into --> cool\n","    x = re.sub(r\"\\b(j+e{2,}z+e*)\\b\", \"jeez\", x)\n","    x = re.sub(r\"\\b(co+l+)\\b\", \"cool\", x)\n","    x = re.sub(r\"\\b(g+o+a+l+)\\b\", \"goal\", x)\n","    x = re.sub(r\"\\b(s+h+i+t+)\\b\", \"shit\", x)\n","    x = re.sub(r\"\\b(o+m+g+)\\b\", \"omg\", x)\n","    x = re.sub(r\"\\b(w+t+f+)\\b\", \"wtf\", x)\n","    x = re.sub(r\"\\b(w+h+a+t+)\\b\", \"what\", x)\n","    x = re.sub(r\"\\b(y+e+y+|y+a+y+|y+e+a+h+)\\b\", \"yeah\", x)\n","    x = re.sub(r\"\\b(w+o+w+)\\b\", \"wow\", x)\n","    x = re.sub(r\"\\b(w+h+y+)\\b\", \"why\", x)\n","    x = re.sub(r\"\\b(s+o+)\\b\", \"so\", x)\n","    x = re.sub(r\"\\b(f)\\b\", \"fuck\", x)\n","    x = re.sub(r\"\\b(w+h+o+p+s+)\\b\", \"whoops\", x)\n","    x = re.sub(r\"\\b(ofc)\\b\", \"of course\", x)\n","    x = re.sub(r\"\\b(the us)\\b\", \"usa\", x)\n","    x = re.sub(r\"\\b(gf)\\b\", \"girlfriend\", x)\n","    x = re.sub(r\"\\b(hr)\\b\", \"human ressources\", x)\n","    x = re.sub(r\"\\b(mh)\\b\", \"mental health\", x)\n","    x = re.sub(r\"\\b(idk)\\b\", \"i do not know\", x)\n","    x = re.sub(r\"\\b(gotcha)\\b\", \"i got you\", x)\n","    x = re.sub(r\"\\b(y+e+p+)\\b\", \"yes\", x)\n","    x = re.sub(r\"\\b(a*ha+h[ha]*|a*ha +h[ha]*)\\b\", \"haha\", x)\n","    x = re.sub(r\"\\b(o?l+o+l+[ol]*)\\b\", \"lol\", x)\n","    x = re.sub(r\"\\b(o*ho+h[ho]*|o*ho +h[ho]*)\\b\", \"ohoh\", x)\n","    x = re.sub(r\"\\b(o+h+)\\b\", \"oh\", x)\n","    x = re.sub(r\"\\b(a+h+)\\b\", \"ah\", x)\n","    x = re.sub(r\"\\b(u+h+)\\b\", \"uh\", x)\n","\n","    # Handling emojis\n","    x = re.sub(r\"<3\", \" love \", x)\n","    x = re.sub(r\"xd\", \" smiling_face_with_open_mouth_and_tightly_closed_eyes \", x)\n","    x = re.sub(r\":\\)\", \" smiling_face \", x)\n","    x = re.sub(r\"^_^\", \" smiling_face \", x)\n","    x = re.sub(r\"\\*_\\*\", \" star_struck \", x)\n","    x = re.sub(r\":\\(\", \" frowning_face \", x)\n","    x = re.sub(r\":\\^\\(\", \" frowning_face \", x)\n","    x = re.sub(r\";\\(\", \" frowning_face \", x)\n","    x = re.sub(r\":\\/\",  \" confused_face\", x)\n","    x = re.sub(r\";\\)\",  \" wink\", x)\n","    x = re.sub(r\">__<\",  \" unamused \", x)\n","    x = re.sub(r\"\\b([xo]+x*)\\b\", \" xoxo \", x)\n","    x = re.sub(r\"\\b(n+a+h+)\\b\", \"no\", x)\n","    \n","    # Handling special cases of text\n","    x = re.sub(r\"h a m b e r d e r s\", \"hamberders\", x)\n","    x = re.sub(r\"b e n\", \"ben\", x)\n","    x = re.sub(r\"s a t i r e\", \"satire\", x)\n","    x = re.sub(r\"y i k e s\", \"yikes\", x)\n","    x = re.sub(r\"s p o i l e r\", \"spoiler\", x)\n","    x = re.sub(r\"thankyou\", \"thank you\", x)\n","    x = re.sub(r\"a^r^o^o^o^o^o^o^o^n^d\", \"around\", x)\n","\n","    # Remove special characters and numbers replace by space + remove double space\n","    x = re.sub(r\"\\b([.]{3,})\",\" dots \", x)\n","    x = re.sub(r\"[^A-Za-z!?_]+\",\" \", x)\n","    x = re.sub(r\"\\b([s])\\b *\",\"\", x)\n","    x = re.sub(r\" +\",\" \", x)\n","    x = x.strip()\n","\n","    return x     "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U90nYuBLuQkQ"},"source":["Now, let's define a function that makes predictions based on a text sample."]},{"cell_type":"code","metadata":{"id":"Bm-6OEi_1V6t"},"source":["def predict_samples(text_samples, model):\n","\n","    # Text preprocessing and cleaning\n","    text_samples = pd.Series(text_samples)\n","    text_samples_clean = text_samples.apply(preprocess_corpus)\n","    \n","    # Create tfidf representation\n","    tfidf_text_samples_clean = vectorizer.transform(text_samples_clean)\n","    \n","    # labels predictions\n","    samples_pred_labels = model.predict(tfidf_text_samples_clean)\n","    samples_pred_labels_df = pd.DataFrame(samples_pred_labels)\n","    samples_pred_labels_df = samples_pred_labels_df.apply(lambda x: [GE_taxonomy[i] for i in range(len(x)) if x[i]==1], axis=1)\n","    \n","    return pd.DataFrame({\"Text\":text_samples, \"Emotions\":list(samples_pred_labels_df)})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQ_dctb0O-9A","colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"status":"ok","timestamp":1617218473739,"user_tz":-120,"elapsed":1580,"user":{"displayName":"Ibrahim Benjelloun","photoUrl":"","userId":"05131578507394582340"}},"outputId":"7711d89d-1509-46ac-8a9c-dee9ba98b173"},"source":["# Predict samples\n","predict_samples(\"no one cares my guy\", classifier)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Text</th>\n","      <th>Emotions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>no one cares my guy</td>\n","      <td>[curiosity, neutral]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  Text              Emotions\n","0  no one cares my guy  [curiosity, neutral]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"aVYWcPhtu8_3"},"source":["# Conclusion"]},{"cell_type":"markdown","metadata":{"id":"dwogMwfhvAnF"},"source":["*   In this notebook, we constructed a dummy model that always predicts the 'Neutral' emotion. Given that this emotion is the most represented, the \"model\" has reasonable performances when it comes to detecting 'Neutral', but has poor global performances.\n","* The baseline model we trained allowed an increase in the score but it is still very low. This can be due to the fact that it considers the words in a text sample only according to their importance, and does not put them in their context (a sample is a combination of independent words)\n","*  In the next step, we are going to be using an algorithme that adresses the latter issue usinf the mechanism of 'attention': The BERT model"]}]}